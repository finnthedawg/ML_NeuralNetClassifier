{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn #neural net\n",
    "import torch.nn.functional as F #Functions such as relu\n",
    "import torch.optim as optim #Parameter optimization functions\n",
    "import numpy as np\n",
    "import random #Random shuffling\n",
    "from scipy.io import loadmat #Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        #Define the network\n",
    "        super(ANN, self).__init__()\n",
    "        self.fc1 = nn.Linear(400, 25)\n",
    "        self.fc2 = nn.Linear(25,10)\n",
    "        self.Sigmoid = nn.Sigmoid()\n",
    "        self.Softmax = nn.Softmax(dim = -1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.Sigmoid(self.fc1(x))\n",
    "        x = self.Softmax(self.fc2(x))\n",
    "        return(x)\n",
    "    \n",
    "    def loadWeight(self, weights):\n",
    "        \n",
    "        fc1 = torch.from_numpy(weights[\"Theta1\"][:,:-1]).float()\n",
    "        fc1Bias = torch.from_numpy(weights[\"Theta1\"][:,0]).float()\n",
    "        fc2 = torch.from_numpy(weights[\"Theta2\"][:,:-1]).float()\n",
    "        fc2Bias = torch.from_numpy(weights[\"Theta2\"][:,0]).float()\n",
    "        \n",
    "        print(fc1.shape)\n",
    "        print(fc1Bias)\n",
    "        print(fc2Bias)\n",
    "        \n",
    "        param = list(self.parameters())\n",
    "        param[0].data.sub_(fc1)\n",
    "        param[1].data.sub_(fc1Bias)\n",
    "        param[2].data.sub_(fc2)\n",
    "        param[3].data.sub_(fc2Bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData(name):\n",
    "    return(loadmat(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:  (5000, 400)\n",
      "Output shape:  (5000, 1)\n"
     ]
    }
   ],
   "source": [
    "data = loadData(\"ex3data1.mat\")\n",
    "print(\"Input shape: \", data[\"X\"].shape)\n",
    "print(\"Output shape: \", data[\"y\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy%:  [94.5]\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the network\n",
    "def evaluate(model, data):\n",
    "    dataSize = len(data[\"X\"][:,1]) #Total number of samples.\n",
    "    #Test on the dataset\n",
    "    #For each of the featureset.\n",
    "    samples = list(range(dataSize))\n",
    "    random.shuffle(samples)\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in samples:\n",
    "            features = torch.from_numpy(data[\"X\"][i]).float()\n",
    "            #Extract the correct value\n",
    "            target = torch.zeros((10))\n",
    "            target[data[\"y\"][i]-1] = 1.0\n",
    "            #Predict\n",
    "            outputs = classifier(features)\n",
    "            _, predicted = torch.max(outputs, 0)\n",
    "            correct += (int(predicted) == data[\"y\"][i]-1)\n",
    "    return(100*correct/dataSize)\n",
    "print(\"Accuracy%: \", evaluate(classifier, data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANN(\n",
      "  (fc1): Linear(in_features=400, out_features=25, bias=True)\n",
      "  (fc2): Linear(in_features=25, out_features=10, bias=True)\n",
      "  (Sigmoid): Sigmoid()\n",
      "  (Softmax): Softmax()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Initialize the neural net\n",
    "classifier = ANN()\n",
    "print(classifier)\n",
    "#Create the optim\n",
    "optimizer = optim.SGD(classifier.parameters(), lr=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Avg MSE Loss: 0.0033221883845615564 Ave Accuracy%:  [98.44]\n",
      "Epoch:1 Avg MSE Loss: 0.003267398532864635 Ave Accuracy%:  [98.48]\n",
      "Epoch:2 Avg MSE Loss: 0.003225699139590652 Ave Accuracy%:  [98.5]\n",
      "Epoch:3 Avg MSE Loss: 0.0031578404628805927 Ave Accuracy%:  [98.54]\n",
      "Epoch:4 Avg MSE Loss: 0.0031113516207628934 Ave Accuracy%:  [98.5]\n",
      "Epoch:5 Avg MSE Loss: 0.0030710928747006002 Ave Accuracy%:  [98.54]\n",
      "Epoch:6 Avg MSE Loss: 0.003024588048180694 Ave Accuracy%:  [98.56]\n",
      "Epoch:7 Avg MSE Loss: 0.0029881792776655933 Ave Accuracy%:  [98.58]\n",
      "Epoch:8 Avg MSE Loss: 0.0029424303247170542 Ave Accuracy%:  [98.58]\n",
      "Epoch:9 Avg MSE Loss: 0.0028983883814656707 Ave Accuracy%:  [98.58]\n"
     ]
    }
   ],
   "source": [
    "#Training\n",
    "dataSize = len(data[\"X\"][:,1]) #Total number of samples.\n",
    "for epoch in range(10):\n",
    "    \n",
    "    lossCount = []\n",
    "    accuracyCount = []\n",
    "    correctCount = 0\n",
    "    #For each of the featureset.\n",
    "    samples = list(range(dataSize))\n",
    "    random.shuffle(samples)\n",
    "    for i in samples:\n",
    "        #Train with SGD for each set. |\n",
    "        optimizer.zero_grad()   # zero the gradient\n",
    "        #Extract a feature i\n",
    "        features = torch.from_numpy(data[\"X\"][i]).float()\n",
    "        #Extract the correct value\n",
    "        target = torch.zeros((10))\n",
    "        target[data[\"y\"][i]-1] = 1.0\n",
    "        #Define the MSE loss.\n",
    "        output = classifier(features)\n",
    "        loss = nn.MSELoss()(output, target)\n",
    "        #Backprop the loss to each node.\n",
    "        loss.backward()\n",
    "        #Updates our weights using optim SGD.\n",
    "        optimizer.step()\n",
    "        #Keep track of loss and accuracy.\n",
    "        lossCount.append(loss)\n",
    "    lossCount = np.array(lossCount, dtype=float)\n",
    "    mean = np.mean(lossCount)\n",
    "    print(\"Epoch:\"+ str(epoch) + \" Avg MSE Loss:\", mean, \"Ave Accuracy%: \", evaluate(classifier, data))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Update the weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy of loaded weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25, 400])\n",
      "tensor([-0.0226, -0.0984,  0.1162, -0.2397, -0.7316, -0.5979,  0.1546, -0.0337,\n",
      "        -0.4107,  0.0235,  0.2477,  0.2653,  0.0943,  0.2022, -0.2030,  0.1046,\n",
      "         0.1489, -0.0379, -0.3320, -0.2977, -0.4842, -0.3898, -0.1832, -0.7021,\n",
      "        -0.3509])\n",
      "tensor([-0.7610, -0.6179, -0.6893, -0.6783, -0.5966, -0.8779, -0.5275, -0.7490,\n",
      "        -0.6665, -0.4609])\n"
     ]
    }
   ],
   "source": [
    "#Load some weights\n",
    "weights = loadData(\"ex3weights.mat\")\n",
    "classifier.loadWeight(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
